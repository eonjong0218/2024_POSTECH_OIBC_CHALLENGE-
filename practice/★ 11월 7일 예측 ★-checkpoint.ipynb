{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad28fd-0847-448a-813d-0591d8bb5ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base_path = 'C:/Users/언종/Desktop/data'\n",
    "\n",
    "# 각 데이터 파일 불러오기\n",
    "weather_forecast_1 = pd.read_csv(f'{base_path}/기상예측데이터_1.csv')\n",
    "weather_forecast_2 = pd.read_csv(f'{base_path}/기상예측데이터_2.csv')\n",
    "weather_actual_1 = pd.read_csv(f'{base_path}/기상실측데이터_1.csv')\n",
    "weather_actual_2 = pd.read_csv(f'{base_path}/기상실측데이터_2.csv')\n",
    "price_day_ahead = pd.read_csv(f'{base_path}/제주전력시장_시장전기가격_하루전가격.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d9683b-4eb6-430a-9d46-9dfd65f5cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "price_day_ahead = price_day_ahead.rename(columns={\n",
    "    '하루전가격(원/kWh)' : 'smp_da'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7958531-5c26-47ca-a7fd-2ed2394317dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2024-10-23', '2024-10-24', '2024-10-25', '2024-10-26', '2024-10-27',\n",
    "         '2024-10-28', '2024-10-29','2024-10-30', '2024-10-31', '2024-11-01',\n",
    "         '2024-11-02', '2024-11-03', '2024-11-04', '2024-11-05','2024-11-06']\n",
    "\n",
    "data_frames = []\n",
    "\n",
    "token = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJqaUtiN01nazVKZThnd3JTbmFmMk1HIiwiaWF0IjoxNzMxMjAxMzE4LCJleHAiOjE3MzE1OTY0MDAsInR5cGUiOiJhcGlfa2V5In0.1HewB2-rCsqpwLN4FlbwPOAOoIDK54phUZmcJorS2W4\"\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {token}'\n",
    "}\n",
    "\n",
    "for date in dates:\n",
    "    response = requests.get(f'https://research-api.solarkim.com/data/cmpt-2024/smp-da/{date}', headers=headers)\n",
    "    daily_data = response.json()\n",
    "    daily_df = pd.DataFrame(daily_data)\n",
    "    data_frames.append(daily_df)\n",
    "\n",
    "smp_da_combined_df = pd.concat(data_frames, ignore_index=True)\n",
    "smp_da_combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f8afd1-2d1c-492c-9066-5a107f940ce1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dates = ['2024-10-23', '2024-10-24', '2024-10-25', '2024-10-26', '2024-10-27',\n",
    "         '2024-10-28', '2024-10-29', '2024-10-30', '2024-10-31', '2024-11-01',\n",
    "         '2024-11-02', '2024-11-03', '2024-11-04', '2024-11-05','2024-11-06']\n",
    "token = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJqaUtiN01nazVKZThnd3JTbmFmMk1HIiwiaWF0IjoxNzMxMjAxMzE4LCJleHAiOjE3MzE1OTY0MDAsInR5cGUiOiJhcGlfa2V5In0.1HewB2-rCsqpwLN4FlbwPOAOoIDK54phUZmcJorS2W4\"\n",
    "headers = {'Authorization': f'Bearer {token}'}\n",
    "\n",
    "def fetch_data(endpoint, date):\n",
    "    url = f'https://research-api.solarkim.com/data/cmpt-2024/{endpoint}/{date}'\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "weather_forecast_1_dfs = []\n",
    "weather_forecast_2_dfs = []\n",
    "\n",
    "for date in dates:\n",
    "    data = fetch_data('weather-forecast', date)\n",
    "    weather_forecast_1_dfs.append(pd.DataFrame(data['weather_forecast_1']))\n",
    "    weather_forecast_2_dfs.append(pd.DataFrame(data['weather_forecast_2']))\n",
    "\n",
    "weather_forecast_1_combined_df = pd.concat(weather_forecast_1_dfs, ignore_index=True)\n",
    "weather_forecast_2_combined_df = pd.concat(weather_forecast_2_dfs, ignore_index=True)\n",
    "\n",
    "display(weather_forecast_1_combined_df)\n",
    "display(weather_forecast_2_combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c574619f-1767-4388-92b6-4336812ea087",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = ['2024-10-23', '2024-10-24', '2024-10-25', '2024-10-26', '2024-10-27',\n",
    "         '2024-10-28', '2024-10-29', '2024-10-30', '2024-10-31', '2024-11-01',\n",
    "         '2024-11-02', '2024-11-03', '2024-11-04', '2024-11-05','2024-11-06']\n",
    "token = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJqaUtiN01nazVKZThnd3JTbmFmMk1HIiwiaWF0IjoxNzMxMjAxMzE4LCJleHAiOjE3MzE1OTY0MDAsInR5cGUiOiJhcGlfa2V5In0.1HewB2-rCsqpwLN4FlbwPOAOoIDK54phUZmcJorS2W4\"\n",
    "headers = {'Authorization': f'Bearer {token}'}\n",
    "\n",
    "def fetch_data(endpoint, date):\n",
    "    url = f'https://research-api.solarkim.com/data/cmpt-2024/{endpoint}/{date}'\n",
    "    response = requests.get(url, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "actual_weather_1_dfs = []\n",
    "actual_weather_2_dfs = []\n",
    "\n",
    "for date in dates:\n",
    "    data = fetch_data('actual-weather', date)\n",
    "    actual_weather_1_dfs.append(pd.DataFrame(data['actual_weather_1']))\n",
    "    actual_weather_2_dfs.append(pd.DataFrame(data['actual_weather_2']))\n",
    "\n",
    "actual_weather_1_combined_df = pd.concat(actual_weather_1_dfs, ignore_index=True)\n",
    "actual_weather_2_combined_df = pd.concat(actual_weather_2_dfs, ignore_index=True)\n",
    "\n",
    "display(actual_weather_1_combined_df)\n",
    "display(actual_weather_2_combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e57a3d-7f4d-4745-8810-02769a93fd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_forecast_1 = pd.concat([weather_forecast_1, weather_forecast_1_combined_df], ignore_index=True)\n",
    "weather_forecast_2 = pd.concat([weather_forecast_2, weather_forecast_2_combined_df], ignore_index=True)\n",
    "weather_actual_1 = pd.concat([weather_actual_1, actual_weather_1_combined_df], ignore_index=True)\n",
    "weather_actual_2 = pd.concat([weather_actual_2, actual_weather_2_combined_df], ignore_index=True)\n",
    "price_day_ahead = pd.concat([price_day_ahead, smp_da_combined_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36278ae2-9227-4b24-8156-9be63dad75ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_columns(df):\n",
    "    for col in df.columns:\n",
    "        if col == 'ts' :\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')  # ts는 Int64형으로 변환\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').astype(float)  # 나머지는 float형으로 변환\n",
    "    return df  \n",
    "\n",
    "weather_forecast_1 = convert_columns(weather_forecast_1)\n",
    "weather_forecast_2 = convert_columns(weather_forecast_2)\n",
    "weather_actual_1 = convert_columns(weather_actual_1)\n",
    "weather_actual_2 = convert_columns(weather_actual_2)\n",
    "price_day_ahead = convert_columns(price_day_ahead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b09fdb-192c-4b14-a793-692de551f105",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_forecast_1 = weather_forecast_1.drop(columns=['location'], errors='ignore')\n",
    "weather_forecast_2 = weather_forecast_2.drop(columns=['location'], errors='ignore')\n",
    "weather_actual_1 = weather_actual_1.drop(columns=['location'], errors='ignore')\n",
    "weather_actual_2 = weather_actual_2.drop(columns=['location'], errors='ignore')\n",
    "\n",
    "weather_forecast_1 = weather_forecast_1.drop(columns=['base_ts'], errors='ignore')\n",
    "weather_forecast_2 = weather_forecast_2.drop(columns=['base_ts'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27e81b-a22e-40dd-98ee-4b0de83ba79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_ts =  1730822400\n",
    "\n",
    "for df in [weather_actual_1, weather_actual_2]:\n",
    "    df.drop(df[df['ts'] > threshold_ts].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f5ea18-2ed3-464c-903a-ce1dee38cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_unique_cols = set(weather_forecast_1.columns) - set(weather_actual_1.columns)\n",
    "actual_unique_cols = set(weather_actual_1.columns) - set(weather_forecast_1.columns)\n",
    "\n",
    "print(\"\\nweather_forecast_1에만 있는 칼럼:\")\n",
    "print(forecast_unique_cols)\n",
    "\n",
    "print(\"\\nweather_actual_1에만 있는 칼럼:\")\n",
    "print(actual_unique_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91e824b-71a5-48a3-ac80-cc0a99ebbb5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 각 데이터 프레임에 대해 결측치가 포함된 행만 출력\n",
    "dataframes = {\n",
    "    \"weather_forecast_1\": weather_forecast_1,\n",
    "    \"weather_forecast_2\": weather_forecast_2,\n",
    "    \"weather_actual_1\": weather_actual_1,\n",
    "    \"weather_actual_2\": weather_actual_2,\n",
    "    \"price_day_ahead\": price_day_ahead\n",
    "}\n",
    "\n",
    "for name, df in dataframes.items():\n",
    "    print(f\"\\nDataFrame: {name}\")\n",
    "    # 결측치가 있는 행만 선택\n",
    "    missing_rows = df[df.isnull().any(axis=1)]\n",
    "    display(missing_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe808b2c-d31d-49da-b770-8e23e82f455a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_missing_rows(df):\n",
    "    return df.dropna(subset=['ts'])\n",
    "\n",
    "weather_forecast_1 = drop_missing_rows(weather_forecast_1)\n",
    "weather_forecast_2 = drop_missing_rows(weather_forecast_2)\n",
    "weather_actual_1 = drop_missing_rows(weather_actual_1)\n",
    "weather_actual_2 = drop_missing_rows(weather_actual_2)\n",
    "price_day_ahead = drop_missing_rows(price_day_ahead)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b546840b-b051-4f03-ba3e-21b78516f6ac",
   "metadata": {},
   "source": [
    "# 주요 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db393752-51e7-4fc5-9b02-091dd324c042",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def round_to_nearest_hour_and_average(df):\n",
    "    df.loc[:, 'ts'] = pd.to_datetime(df['ts'], unit='s') + pd.Timedelta(hours=9)\n",
    "    \n",
    "    df.loc[:, 'rounded_ts'] = df['ts'].dt.round('h')\n",
    "    \n",
    "    df_mean = df.groupby('rounded_ts').mean().reset_index()\n",
    "\n",
    "    df_mean['ts'] = (df_mean['rounded_ts'].astype('int64') // 10**9).astype(int) \n",
    "\n",
    "    df_mean.drop(columns=['rounded_ts'], inplace=True)\n",
    "    \n",
    "    return df_mean\n",
    "\n",
    "weather_actual_1 = round_to_nearest_hour_and_average(weather_actual_1)\n",
    "weather_actual_2 = round_to_nearest_hour_and_average(weather_actual_2)\n",
    "\n",
    "display(weather_actual_1)\n",
    "display(weather_actual_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3901203a-34cb-47c9-b8d1-d07bc9d74e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [\n",
    "    weather_forecast_1,\n",
    "    weather_forecast_2,\n",
    "    weather_actual_1,\n",
    "    weather_actual_2\n",
    "]\n",
    "\n",
    "merged_df = price_day_ahead.copy()\n",
    "\n",
    "for df in dataframes:\n",
    "    df = df.drop_duplicates(subset=['ts'])\n",
    "\n",
    "    merged_df = pd.merge(merged_df, df, on='ts', how='left', suffixes=('', '_dup'))\n",
    "\n",
    "    for column in df.columns:\n",
    "        if column != 'ts' and column in merged_df.columns and f\"{column}_dup\" in merged_df.columns:\n",
    "            merged_df[column] = merged_df[[column, f\"{column}_dup\"]].mean(axis=1)\n",
    "            merged_df.drop(columns=[f\"{column}_dup\"], inplace=True)\n",
    "\n",
    "print(merged_df.shape)\n",
    "display(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a81f3b-f2c8-42f1-b5d6-c98a2072fc53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_rows = merged_df[merged_df.isna().any(axis=1)]\n",
    "missing_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272fd2d7-2599-4ebc-a8bf-083507ae8460",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['appr_temp', 'precip_1h', 'pressure',\n",
    "                   'real_feel_temp_shade', 'wind_chill_temp','ice',\n",
    "                   'snow_prob', 'snow', 'ice_prob']\n",
    "\n",
    "merged_df = merged_df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce249e0a-102a-44e8-a4b4-6721a7dead45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_rows = merged_df[merged_df.isna().any(axis=1)]\n",
    "missing_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67b59b8b-ddf7-4a35-8865-997838d77fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = merged_df.shape[1] / 2 \n",
    "merged_df = merged_df.dropna(thresh=threshold).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020dd29b-8c28-46cb-9c31-444f1aec3744",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_rows = merged_df[merged_df.isna().any(axis=1)]\n",
    "missing_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d7272-f7dd-4a04-82aa-80010dffb2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['datetime'] = pd.to_datetime(merged_df['ts'], unit='s', utc=True).dt.tz_convert('Asia/Seoul')\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad75938-2a5f-4d7b-b319-e4ad07055671",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_columns = merged_df.columns[merged_df.isnull().any()].tolist()\n",
    "\n",
    "print(\"결측치가 있는 칼럼들:\")\n",
    "print(missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8851da0a-95c2-4bac-898d-afee8e8021e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_to_fill = ['precip_prob', 'rain_prob', 'total_liq']\n",
    "\n",
    "for col in columns_to_fill:\n",
    "    for i in merged_df.index:\n",
    "        if pd.isna(merged_df.loc[i, col]):\n",
    "            current_time = merged_df.loc[i, 'datetime']\n",
    "            \n",
    "            previous_day_time = current_time - pd.Timedelta(days=1)\n",
    "            \n",
    "            previous_day_data = merged_df[merged_df['datetime'] == previous_day_time]\n",
    "            \n",
    "            if not previous_day_data.empty:\n",
    "                merged_df.loc[i, col] = previous_day_data[col].values[0]\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdb5d4b-49b7-4117-9607-ccafd4d44b4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_rows = merged_df[merged_df.isna().any(axis=1)]\n",
    "missing_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ea9e8-3cb5-4979-8912-1f13f5b6a136",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_columns = merged_df.columns[merged_df.isnull().any()].tolist()\n",
    "\n",
    "print(\"결측치가 있는 칼럼들:\")\n",
    "print(missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e87c6b4-9bf1-4e56-8ca3-f59a6355dc7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns_to_fill = [\n",
    "   'precip_prob', 'rain_prob', 'total_liq'\n",
    "]\n",
    "\n",
    "for col in columns_to_fill:\n",
    "    for i in merged_df.index:\n",
    "        if pd.isna(merged_df.loc[i, col]):  \n",
    "            current_time = merged_df.loc[i, 'datetime']\n",
    "            \n",
    "            next_day_time = current_time + pd.Timedelta(days=1)\n",
    "            \n",
    "            next_day_data = merged_df[merged_df['datetime'] == next_day_time]\n",
    "            \n",
    "            if not next_day_data.empty:\n",
    "                merged_df.loc[i, col] = next_day_data[col].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0337c5b6-c522-4e07-8c6a-d200f44568fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_columns = merged_df.columns[merged_df.isnull().any()].tolist()\n",
    "\n",
    "print(\"결측치가 있는 칼럼들:\")\n",
    "print(missing_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998b584c-b315-4ed2-8696-ac54ace57272",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "merged_df['hour'] = merged_df['datetime'].dt.hour\n",
    "merged_df['month'] = merged_df['datetime'].dt.month\n",
    "merged_df['day'] = merged_df['datetime'].dt.day\n",
    "\n",
    "merged_df['hour_sin'] = np.sin(2 * np.pi * merged_df['hour'] / 24)\n",
    "\n",
    "merged_df = merged_df.drop(columns=['hour'])\n",
    "\n",
    "merged_df = merged_df.drop(columns=['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f50a804-cb9d-4116-b4f3-8a82b985a0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.copy()\n",
    "\n",
    "merged_df['date'] = pd.to_datetime(merged_df['ts'], unit='s', utc=True).dt.tz_convert('Asia/Seoul')\n",
    "merged_df['weekday'] = merged_df['date'].dt.dayofweek  \n",
    "\n",
    "merged_df['월요일'] = (merged_df['weekday'] == 0).astype(int)  \n",
    "merged_df['평일'] = merged_df['weekday'].isin(range(1, 5)).astype(int)  \n",
    "merged_df['주말'] = merged_df['weekday'].isin([5, 6]).astype(int) \n",
    "\n",
    "merged_df['공휴일'] = 0\n",
    "\n",
    "merged_df = merged_df.drop(columns=['date', 'weekday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692cde7f-f351-453e-80b4-dd4824258c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "holiday_dates = [\n",
    "    '2024-03-01', '2024-05-05', '2024-05-15', '2024-06-06',\n",
    "    '2024-08-15', '2024-09-16', '2024-09-17', '2024-09-18',\n",
    "    '2024-10-03', '2024-10-09'\n",
    "]\n",
    "\n",
    "merged_df['date'] = pd.to_datetime(merged_df['ts'], unit='s', utc=True).dt.tz_convert('Asia/Seoul')\n",
    "merged_df['공휴일'] = merged_df['date'].dt.strftime('%Y-%m-%d').isin(holiday_dates).astype(int)\n",
    "\n",
    "merged_df = merged_df.drop(columns=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8298c0-d717-43a7-ac23-2a1f391de1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.rename(columns={\n",
    "    '월요일': 'Monday',\n",
    "    '평일': 'Weekday',\n",
    "    '주말': 'Weekend',\n",
    "    '공휴일': 'Holiday'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1949517d-8eab-43b1-8a64-a322db19a47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_max 칼럼 삭제\n",
    "merged_df = merged_df.drop(columns=['temp_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea5c81c-b653-4829-90a4-b82b94c45707",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6e9e69-d330-4a2c-ac02-63c79f5ad73c",
   "metadata": {},
   "source": [
    "# ***예측날 테이블 만들기***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fb4f16-f060-4e78-a61d-6a38d98df4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "date = '2024-11-06'\n",
    "token = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJqaUtiN01nazVKZThnd3JTbmFmMk1HIiwiaWF0IjoxNzMxMjAxMzE4LCJleHAiOjE3MzE1OTY0MDAsInR5cGUiOiJhcGlfa2V5In0.1HewB2-rCsqpwLN4FlbwPOAOoIDK54phUZmcJorS2W4\"\n",
    "\n",
    "weather_forecast = requests.get(f'https://research-api.solarkim.com/data/cmpt-2024/weather-forecast/{date}', headers={\n",
    "    'Authorization': f'Bearer {token}'\n",
    "}).json()\n",
    "\n",
    "df_weather_forecast_1 = pd.DataFrame(weather_forecast['weather_forecast_1'])\n",
    "df_weather_forecast_2 = pd.DataFrame(weather_forecast['weather_forecast_2'])\n",
    "\n",
    "df_weather_forecast_1 = df_weather_forecast_1.drop(columns=['location', 'base_ts'], errors='ignore')\n",
    "df_weather_forecast_2 = df_weather_forecast_2.drop(columns=['location', 'base_ts'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75655666-845c-4bb8-8d0a-a6efb07789f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_ts = 1730908800\n",
    "\n",
    "timestamps = [start_ts + i * 3600 for i in range(24)]\n",
    "\n",
    "new_data = {\n",
    "    'ts': timestamps\n",
    "}\n",
    "new_df = pd.DataFrame(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558995ce-0b47-42aa-8f16-06df63976472",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_weather_forecast_1_grouped = df_weather_forecast_1.groupby('ts').mean().reset_index()\n",
    "df_weather_forecast_2_grouped = df_weather_forecast_2.groupby('ts').mean().reset_index()\n",
    "\n",
    "new_df = pd.merge(new_df, df_weather_forecast_1_grouped, on='ts', how='left', suffixes=('', '_1'))\n",
    "new_df = pd.merge(new_df, df_weather_forecast_2_grouped, on='ts', how='left', suffixes=('', '_2'))\n",
    "\n",
    "for column in new_df.columns:\n",
    "    if '_1' in column or '_2' in column:\n",
    "        base_column = column.replace('_1', '').replace('_2', '')\n",
    "        if base_column in new_df.columns:\n",
    "            new_df[base_column] = new_df[[base_column, column]].mean(axis=1)\n",
    "\n",
    "new_df = new_df.loc[:, ~new_df.columns.str.endswith(('_1', '_2'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3dec50-c5fd-4119-b132-988b9c23d34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['ice','snow_prob', 'snow', 'ice_prob']\n",
    "\n",
    "new_df = new_df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77853d00-71cd-476c-b997-44324c0ad103",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['datetime'] = pd.to_datetime(new_df['ts'], unit='s', utc=True).dt.tz_convert('Asia/Seoul')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910f05f7-5407-45e6-a7c6-e294a5158906",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['hour'] = new_df['datetime'].dt.hour\n",
    "new_df['month'] = new_df['datetime'].dt.month\n",
    "new_df['day'] = new_df['datetime'].dt.day\n",
    "\n",
    "new_df['hour_sin'] = np.sin(2 * np.pi * new_df['hour'] / 24)\n",
    "\n",
    "new_df = new_df.drop(columns=['hour'])\n",
    "\n",
    "new_df = new_df.drop(columns=['datetime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41959721-8754-41a4-873c-e7cae2288fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.copy()\n",
    "\n",
    "new_df['date'] = pd.to_datetime(new_df['ts'], unit='s', utc=True).dt.tz_convert('Asia/Seoul')\n",
    "new_df['weekday'] = new_df['date'].dt.dayofweek  \n",
    "\n",
    "new_df['월요일'] = (new_df['weekday'] == 0).astype(int)  \n",
    "new_df['평일'] = new_df['weekday'].isin(range(1, 5)).astype(int)  \n",
    "new_df['주말'] = new_df['weekday'].isin([5, 6]).astype(int) \n",
    "\n",
    "new_df['공휴일'] = 0\n",
    "\n",
    "new_df = new_df.drop(columns=['date', 'weekday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b12062b-f5a7-4548-ae6c-ffd225dd0d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.rename(columns={\n",
    "    '월요일': 'Monday',\n",
    "    '평일': 'Weekday',\n",
    "    '주말': 'Weekend',\n",
    "    '공휴일': 'Holiday'\n",
    "}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d00877-1277-44c9-82dc-f7c783b0d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = new_df.drop(columns=['temp_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b99c76e-0dc5-4094-aaec-84a4e61639cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81645c4b-5ad7-42af-85f8-e13bbd732165",
   "metadata": {},
   "source": [
    "# ***대회 평가지표***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3525a86-d2b8-4e4b-a1c4-611d40f2767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_measure(actual, forecast):\n",
    "    actual = np.array(actual)\n",
    "    forecast = np.array(forecast)\n",
    "\n",
    "    positive_index = actual > 0\n",
    "    negative_index = actual <= 0\n",
    "\n",
    "    # actual은 0과 -1 사이의 값을 처리\n",
    "    actual[(actual <= 0) & (actual > -1)] = -1\n",
    "    \n",
    "    # 긍정적 및 부정적 가격의 수\n",
    "    n1 = np.sum(positive_index) + 1e-7\n",
    "    n2 = np.sum(negative_index) + 1e-7\n",
    "\n",
    "    # e1: 긍정적 가격 예측 오차 비율\n",
    "    e1 = (\n",
    "        np.sum(\n",
    "            np.abs(actual[positive_index] - forecast[positive_index])\n",
    "            / np.abs(actual[positive_index])\n",
    "        )\n",
    "        / n1\n",
    "    )\n",
    "\n",
    "    # e2: 부정적 가격 예측 오차 비율\n",
    "    e2 = (\n",
    "        np.sum(\n",
    "            np.abs(actual[negative_index] - forecast[negative_index])\n",
    "            / np.abs(actual[negative_index])\n",
    "        )\n",
    "        / n2\n",
    "    )\n",
    "\n",
    "    TP = np.sum((forecast > 0) & (actual > 0))\n",
    "    TN = np.sum((forecast <= 0) & (actual <= 0))\n",
    "    FP = np.sum((forecast > 0) & (actual <= 0))\n",
    "    FN = np.sum((forecast <= 0) & (actual > 0))\n",
    "\n",
    "    # 정확도 계산\n",
    "    Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    print(f'Accuracy: {Accuracy}')\n",
    "    print(f'e1: {e1}, e2: {e2}')\n",
    "\n",
    "    e_F = 0.2 * e1 + 0.8 * e2 - (Accuracy - 0.95)\n",
    "\n",
    "    return e_F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c87e764-eafe-49fd-9294-f08b963d2d72",
   "metadata": {},
   "source": [
    "# ***예측 모델 돌리기***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d941abd6-deed-4e19-9be0-37d551e9c798",
   "metadata": {},
   "source": [
    "# ***SVR + lightGBM***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87331c32-4fab-4c1d-8a8a-08bfbcd70779",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 특성 및 목표 변수 정의\n",
    "target = 'smp_da'\n",
    "features = merged_df.columns.difference([target])\n",
    "\n",
    "# 훈련 데이터와 검증 데이터 분리\n",
    "train_data = merged_df.iloc[:-72]\n",
    "valid_data = merged_df.iloc[-72:]\n",
    "\n",
    "# 특성과 타겟 변수 분리\n",
    "X_train = train_data[features]\n",
    "y_train = train_data[target]\n",
    "X_valid = valid_data[features]\n",
    "y_valid = valid_data[target]\n",
    "\n",
    "# MinMax 스케일링 적용\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_valid_scaled = scaler.transform(X_valid)\n",
    "X_new_scaled = scaler.transform(new_df[features])  # new_df에 스케일 적용\n",
    "\n",
    "# LightGBM 모델 훈련 및 예측\n",
    "lgbm_model = LGBMRegressor(objective='regression', random_state=42)\n",
    "lgbm_model.fit(X_train_scaled, y_train)\n",
    "y_train_pred_lgbm = lgbm_model.predict(X_train_scaled)\n",
    "y_valid_pred_lgbm = lgbm_model.predict(X_valid_scaled)\n",
    "y_new_pred_lgbm = lgbm_model.predict(X_new_scaled)\n",
    "\n",
    "# LightGBM 예측 값과 실제 값의 잔차 계산\n",
    "residual_train = y_train - y_train_pred_lgbm\n",
    "\n",
    "# SVR 모델을 잔차 보정 모델로 훈련 (LightGBM 예측 값을 입력으로 사용)\n",
    "svr_model_corrector = SVR(kernel='rbf', C=50, epsilon=2)\n",
    "svr_model_corrector.fit(y_train_pred_lgbm.reshape(-1, 1), residual_train)\n",
    "\n",
    "# SVR 모델로 잔차 보정 예측\n",
    "residual_valid_pred_svr = svr_model_corrector.predict(y_valid_pred_lgbm.reshape(-1, 1))\n",
    "residual_new_pred_svr = svr_model_corrector.predict(y_new_pred_lgbm.reshape(-1, 1))\n",
    "\n",
    "# 최종 예측 (LightGBM 예측 + SVR로 예측한 잔차 보정)\n",
    "y_valid_final_pred = y_valid_pred_lgbm + residual_valid_pred_svr\n",
    "y_new_final_pred = y_new_pred_lgbm + residual_new_pred_svr\n",
    "\n",
    "# 검증 데이터 평가\n",
    "measure = calculate_measure(y_valid, y_valid_final_pred)\n",
    "print(\"Custom Measure for Validation (Residual Corrector Ensemble):\", measure)\n",
    "\n",
    "# 최종 예측 출력\n",
    "print(\"Predicted SMP for 2024-11-07 (24 hours, Residual Corrector Ensemble):\", y_new_final_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5df6ab1-96cb-4c23-a656-0d6644fce45a",
   "metadata": {},
   "source": [
    "# ***교차검증***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ac666-3275-4968-8e91-cc28dfdf302f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# 특성 및 목표 변수 정의\n",
    "target = 'smp_da'\n",
    "features = merged_df.columns.difference([target])\n",
    "\n",
    "# 데이터 블록을 나누기 위한 파라미터\n",
    "n_splits = 10  # 블록의 개수\n",
    "block_size = len(merged_df) // n_splits  # 각 블록의 크기\n",
    "\n",
    "# MinMax 스케일링 설정\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# 예측 결과를 저장할 리스트\n",
    "all_valid_preds = []  # 검증 데이터 예측값\n",
    "all_new_preds = []    # 새로운 데이터 예측값\n",
    "measures = []         # 평가 결과 저장\n",
    "\n",
    "for i in range(n_splits):\n",
    "    # 블록마다 훈련 및 검증 데이터 설정\n",
    "    start_idx = i * block_size\n",
    "    end_idx = (i + 1) * block_size if (i + 1) < n_splits else len(merged_df)\n",
    "\n",
    "    # 훈련 데이터와 검증 데이터 분리\n",
    "    train_data = pd.concat([merged_df.iloc[:start_idx], merged_df.iloc[end_idx:]])\n",
    "    valid_data = merged_df.iloc[start_idx:end_idx]\n",
    "\n",
    "    # 특성과 타겟 변수 분리\n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data[target]\n",
    "    X_valid = valid_data[features]\n",
    "    y_valid = valid_data[target]\n",
    "\n",
    "    # MinMax 스케일링 적용\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_valid_scaled = scaler.transform(X_valid)\n",
    "    X_new_scaled = scaler.transform(new_df[features])  # new_df에 스케일 적용\n",
    "\n",
    "    # LightGBM 모델 훈련 및 예측\n",
    "    lgbm_model = LGBMRegressor(objective='regression', random_state=42)\n",
    "    lgbm_model.fit(X_train_scaled, y_train)\n",
    "    y_train_pred_lgbm = lgbm_model.predict(X_train_scaled)\n",
    "    y_valid_pred_lgbm = lgbm_model.predict(X_valid_scaled)\n",
    "    y_new_pred_lgbm = lgbm_model.predict(X_new_scaled)\n",
    "\n",
    "    # LightGBM 예측 값과 실제 값의 잔차 계산\n",
    "    residual_train = y_train - y_train_pred_lgbm\n",
    "\n",
    "    # SVR 모델을 잔차 보정 모델로 훈련 (LightGBM 예측 값을 입력으로 사용)\n",
    "    svr_model_corrector = SVR(kernel='rbf', C=50, epsilon=2)\n",
    "    svr_model_corrector.fit(y_train_pred_lgbm.reshape(-1, 1), residual_train)\n",
    "\n",
    "    # SVR 모델로 잔차 보정 예측\n",
    "    residual_valid_pred_svr = svr_model_corrector.predict(y_valid_pred_lgbm.reshape(-1, 1))\n",
    "    residual_new_pred_svr = svr_model_corrector.predict(y_new_pred_lgbm.reshape(-1, 1))\n",
    "\n",
    "    # 최종 예측 (LightGBM 예측 + SVR로 예측한 잔차 보정)\n",
    "    y_valid_final_pred = y_valid_pred_lgbm + residual_valid_pred_svr\n",
    "    y_new_final_pred = y_new_pred_lgbm + residual_new_pred_svr\n",
    "\n",
    "    # 검증 데이터 예측값과 평가 결과 저장\n",
    "    all_valid_preds.extend(y_valid_final_pred)\n",
    "    all_new_preds.append(y_new_final_pred)\n",
    "    measure = mean_absolute_error(y_valid, y_valid_final_pred)\n",
    "    measures.append(measure)\n",
    "    print(f\"Block {i+1}/{n_splits} - Custom Measure (MAE) for Validation (Residual Corrector Ensemble):\", measure)\n",
    "\n",
    "# 블록 별 성능의 평균 계산\n",
    "average_measure = np.mean(measures)\n",
    "print(\"Average Custom Measure across Blocks:\", average_measure)\n",
    "\n",
    "# 최종 예측값 출력 (new_df에 대한 예측)\n",
    "final_new_pred = np.mean(all_new_preds, axis=0)\n",
    "print(\"Predicted SMP for 2024-11-11 (24 hours, Residual Corrector Ensemble):\", final_new_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6af7283-508b-4c73-a7b2-6c2cce5ba0be",
   "metadata": {},
   "source": [
    "# ***예측날 값 불러오기***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86d96d4-0ad7-4a1c-8fc1-8ff8ad59c237",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "date = '2024-11-07'\n",
    "token = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJqaUtiN01nazVKZThnd3JTbmFmMk1HIiwiaWF0IjoxNzMxMjAxMzE4LCJleHAiOjE3MzE1OTY0MDAsInR5cGUiOiJhcGlfa2V5In0.1HewB2-rCsqpwLN4FlbwPOAOoIDK54phUZmcJorS2W4\"\n",
    "\n",
    "smp_da = requests.get(f'https://research-api.solarkim.com/data/cmpt-2024/smp-da/{date}', headers={\n",
    "                            'Authorization': f'Bearer {token}'\n",
    "                        }).json()\n",
    "smp_da = pd.DataFrame(smp_da)\n",
    "smp_da"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8388df35-23b2-4e2b-ab11-52a84a2392c5",
   "metadata": {},
   "source": [
    "# ***그래프 비교***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55382459-2eb3-44ac-bcbd-5eb5c9ba855b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timezone, timedelta\n",
    "\n",
    "# Updated data1 values\n",
    "data1 = {\n",
    "    'ts': [\n",
    "        1730908800, 1730912400, 1730916000, 1730919600, 1730923200, 1730926800,\n",
    "        1730930400, 1730934000, 1730937600, 1730941200, 1730944800, 1730948400,\n",
    "        1730952000, 1730955600, 1730959200, 1730962800, 1730966400, 1730970000,\n",
    "        1730973600, 1730977200, 1730980800, 1730984400, 1730988000, 1730991600\n",
    "    ],\n",
    "    'smp_da': [\n",
    "        103.65, 89.39, 89.39, 89.48, 89.48, 102.16, 114.48, 107.74, 117.84,\n",
    "        136.21, 126.43, 136.87, 116.13, 116.84, 116.84, 116.84, 151.02, 153.68,\n",
    "        117.44, 126.95, 114.94, 107.66, 111.71, 104.64\n",
    "    ]\n",
    "}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "# New data2 values\n",
    "data2 = {\n",
    "    'ts': df1['ts'],  # using the updated ts values\n",
    "    'smp_da': [\n",
    "        91.47240736, 99.04974029, 99.28393949, 97.90760919, 101.65347624,\n",
    "        99.12526561, 101.18990194, 122.17299703, 118.55434294, 91.45657328,\n",
    "        103.51456642, 93.48775225, 99.45628948, 110.69130278, 116.66287464,\n",
    "        124.64667813, 118.95564467, 113.97928774, 121.470205, 121.47884467,\n",
    "        114.80692608, 114.49213246, 117.12267666, 107.68513328\n",
    "    ]\n",
    "}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# Convert timestamps to KST\n",
    "kst = timezone(timedelta(hours=9))\n",
    "df1['ts'] = pd.to_datetime(df1['ts'], unit='s', utc=True).dt.tz_convert(kst)\n",
    "df2['ts'] = pd.to_datetime(df2['ts'], unit='s', utc=True).dt.tz_convert(kst)\n",
    "\n",
    "# Plotting the comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df1['ts'], df1['smp_da'], label='Updated DataFrame 1 (df1)', marker='o')\n",
    "plt.plot(df2['ts'], df2['smp_da'], label='Updated DataFrame 2 (df2)', marker='o')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Timestamp (KST)')\n",
    "plt.ylabel('smp_da Value')\n",
    "plt.title('Comparison of smp_da between updated df1 and df2 with new data')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9114997-f26e-4046-9bb4-be89c7d1b9f9",
   "metadata": {},
   "source": [
    "# ***e_F 계산***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d0c944-c823-4509-b1bb-9ac7731c510e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = {\n",
    "    'ts': [\n",
    "        1730908800, 1730912400, 1730916000, 1730919600, 1730923200, 1730926800,\n",
    "        1730930400, 1730934000, 1730937600, 1730941200, 1730944800, 1730948400,\n",
    "        1730952000, 1730955600, 1730959200, 1730962800, 1730966400, 1730970000,\n",
    "        1730973600, 1730977200, 1730980800, 1730984400, 1730988000, 1730991600\n",
    "    ],\n",
    "    'smp_da': [\n",
    "        103.65, 89.39, 89.39, 89.48, 89.48, 102.16, 114.48, 107.74, 117.84,\n",
    "        136.21, 126.43, 136.87, 116.13, 116.84, 116.84, 116.84, 151.02, 153.68,\n",
    "        117.44, 126.95, 114.94, 107.66, 111.71, 104.64\n",
    "    ]\n",
    "}\n",
    "df1 = pd.DataFrame(data1)\n",
    "\n",
    "# data2 값, ts는 data1과 동일하게 유지\n",
    "data2 = {\n",
    "    'ts': df1['ts'],  # data1의 ts 값을 그대로 가져옴\n",
    "    'smp_da': [\n",
    "        92.36655998, 96.08655024, 97.96964736, 96.85000752, 95.79452404,\n",
    "95.9559948, 94.98637812, 98.40069005, 94.8006717, 65.85702818,\n",
    "59.93593043, 4.34836031, 27.37427809, 89.7628238, 111.78696553,\n",
    "139.69295592, 135.78001139, 137.66192588, 142.25652745, 142.25652745,\n",
    "140.27421497, 143.33970241, 121.52372043, 116.03381746\n",
    "\n",
    "    ]\n",
    "}\n",
    "df2 = pd.DataFrame(data2)\n",
    "\n",
    "# ts를 KST로 변환\n",
    "kst = timezone(timedelta(hours=9))\n",
    "df1['ts'] = pd.to_datetime(df1['ts'], unit='s', utc=True).dt.tz_convert(kst)\n",
    "df2['ts'] = pd.to_datetime(df2['ts'], unit='s', utc=True).dt.tz_convert(kst)\n",
    "\n",
    "# e_F 값 계산\n",
    "e_f_value = calculate_measure(df1['smp_da'], df2['smp_da'])\n",
    "print(f'e_F: {e_f_value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad25b60-ef1d-42a7-aae1-a171d8853969",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
